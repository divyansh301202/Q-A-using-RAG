{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install PyPDF2\n",
    "%pip install langchain\n",
    "%pip install langchain_community\n",
    "%pip install langchain_google_genai\n",
    "%pip install langchain_text_splitters\n",
    "%pip install sentence-transformers\n",
    "%pip install faiss-cpu\n",
    "%pip install cohere\n",
    "%pip install langchain-huggingface\n",
    "%pip install jupyter ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b12818",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary sqlalchemy pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9946a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.llms import Cohere\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts  import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752c129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6658201",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unstructured[all-docs]\" --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "518f7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026d2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # File to keep track of already processed PDFs\n",
    "# PROCESSED_FILE_RECORD = \"processed_files.json\"\n",
    "\n",
    "# # Load already processed files if exists\n",
    "# if os.path.exists(PROCESSED_FILE_RECORD):\n",
    "#     with open(PROCESSED_FILE_RECORD, \"r\") as f:\n",
    "#         processed_files = json.load(f)\n",
    "# else:\n",
    "#     processed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_files = [\n",
    "\n",
    "#     (\"Text.pdf\", \"C:/Users/HP/Documents/GitHub/Q_A/Text.pdf\"),\n",
    "#     (\"Text2.pdf\", \"C:/Users/HP/Documents/GitHub/Q_A/Text2.pdf\"),\n",
    "#     (\"Text3.pdf\", \"C:/Users/HP/Documents/GitHub/Q_A/Text3.pdf\"),\n",
    "#     (\"Text4.pdf\", \"C:/Users/HP/Documents/GitHub/Q_A/Text4.pdf\")\n",
    "  \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b09711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: pyq.pdf\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Processing: Text.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Text2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Text3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Processing: Text4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Total chunks created: 366\n"
     ]
    }
   ],
   "source": [
    "# Set directory containing your PDFs\n",
    "pdf_dir = \"C:/Users/HP/Documents/GitHub/Q_A\"\n",
    "pdf_elements = []\n",
    "\n",
    "# Read all PDFs\n",
    "files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "for file in files:\n",
    "    path = os.path.join(pdf_dir, file)\n",
    "    print(f\"Processing: {file}\")\n",
    "\n",
    "    # Partition the PDF into elements\n",
    "    elements = partition_pdf(\n",
    "        filename=path,\n",
    "        extract_images_in_pdf=True,\n",
    "        strategy=\"fast\",                     # Fast model for layout detection\n",
    "        fast_model_name=\"yolox\",             # Fast layout model\n",
    "        infer_table_structure=True,          # Extract tables properly\n",
    "        chunking_strategy=\"by_title\",        # Use heading-based chunking\n",
    "        max_characters=1000,\n",
    "        #new_after_n_chars=800,\n",
    "        multipage_sectioning=True,  # Combine text across pages\n",
    "        combine_text_under_n_chars=200,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Add metadata and store\n",
    "    for e in elements:\n",
    "        page_no = getattr(e.metadata, \"page_number\", \"unknown\")\n",
    "        e.metadata.source = file\n",
    "        e.metadata.page_no = page_no\n",
    "\n",
    "        pdf_elements.append(e)\n",
    "\n",
    "print(f\"Total chunks created: {len(pdf_elements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a3c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\GitHub\\Q_A\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:1096: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    }
   ],
   "source": [
    "# CONNECTION_STRING = \"postgresql+psycopg2://postgres:postgres@localhost:5433/rag_db\"\n",
    "\n",
    "# vectorstore = PGVector.from_existing_index(\n",
    "#     collection_name=\"rag_chunks\",\n",
    "#     connection_string=CONNECTION_STRING,\n",
    "#     embedding=embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9003f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Convert unstructured elements into LangChain Documents\n",
    "docs = []\n",
    "\n",
    "for element in pdf_elements:\n",
    "    \n",
    "     # Clean text to remove null characters\n",
    "    clean_text = element.text.replace('\\x00', '').strip()\n",
    "    metadata = {\n",
    "        \"source\": getattr(element.metadata, \"source\", \"unknown\"),\n",
    "        \"page_no\": getattr(element.metadata, \"page_no\", \"unknown\")\n",
    "    }\n",
    "    docs.append(Document(page_content=clean_text, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca9b0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be4b06d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\GitHub\\Q_A\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:490: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define connection string and create new index (only run ONCE unless you want to overwrite)\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://postgres:postgres@localhost:5433/rag_db\"\n",
    "\n",
    "vectorstore = PGVector.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"unstructured_chunks\",  # create a new index name\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27036ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_splitter = RecursiveCharacterTextSplitter(\n",
    "        #chunk_size = 1000, \n",
    "        #chunk_overlap = 200, # This is helpul to handle the data loss while chunking.\n",
    "       # length_function = len,\n",
    "       # separators=['\\n', '\\n\\n', ' ', '']\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already processed: Text.pdf\n",
      "Skipping already processed: Text2.pdf\n",
      "Skipping already processed: Text3.pdf\n",
      "Skipping already processed: Text4.pdf\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# docs=[]\n",
    "# for pdf_name, pdf_path in pdf_files:\n",
    "#     if pdf_name in processed_files:\n",
    "#         print(f\"Skipping already processed: {pdf_name}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     with open(pdf_path, 'rb') as f:\n",
    "#         reader = PdfReader(f)\n",
    "#         page_text = \"\"\n",
    "#         for i, page in enumerate(reader.pages):\n",
    "#             page_text += page.extract_text()\n",
    "            \n",
    "\n",
    "#         # Split page text into chunks\n",
    "#         chunks = text_splitter.split_text(page_text)\n",
    "\n",
    "\n",
    "#         # Add metadata to each chunks\n",
    "#         for chunk in chunks:\n",
    "#             doc = Document(\n",
    "#                 page_content=chunk,\n",
    "#                 metadata={\n",
    "#                     \"source\": pdf_name,        # e.g., \"Text2.pdf\"\n",
    "#                     \"page_no\": chunks.index(chunk) + 1  # Chunk numbers start from\n",
    "\n",
    "#                 }\n",
    "#             )\n",
    "#             docs.append(doc)\n",
    "\n",
    "#     # Add file to processed list\n",
    "#     processed_files.append(pdf_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed files list to JSON\n",
    "# with open(PROCESSED_FILE_RECORD, \"w\") as f:\n",
    "#     json.dump(processed_files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NULL characters from each doc's content\n",
    "# for doc in docs:\n",
    "#     doc.page_content = doc.page_content.replace('\\x00', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c572cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new documents to embed.\n"
     ]
    }
   ],
   "source": [
    "#Embed Only New Chunks\n",
    "# if docs:\n",
    "#     vectorstore.add_documents(docs)\n",
    "#     print(f\"{len(docs)} new chunks added to vectorstore.\")\n",
    "# else:\n",
    "#     print(\"No new documents to embed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73dc4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore = FAISS.from_texts(chunks, embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36f91b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5982005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a helpful assistant. Use the provided context below to answer the user's question strictly from the specified files.\n",
    "Each context chunk includes metadata like `source` and `page_number`.\n",
    "\n",
    "\n",
    "Provide answer(s) ONLY from the context. If no relevant answer is found, respond exactly with:\n",
    "{{\"answer\": \"answer not available in context\"}}\n",
    "\n",
    "Otherwise, your output must be STRICTLY of a JSON list of objects with this structure:\n",
    "[\n",
    "{{\n",
    "\"answer\": \"A part of answer to the question from a particular chunk.\",\n",
    "\"source\": \"The source of the chunk from which this part of the answer is found\",\n",
    "\"page_number\": the page number of the chunk from which this part of the answer is found\n",
    "}}\n",
    "]\n",
    "Context chunks (with metadata):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a30a501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\hp\\documents\\github\\q_a\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3911202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for multiple PDFs (comma-separated input)\n",
    "selected_pdfs = input(\"Enter the PDF file names: \").strip().split(',')\n",
    "\n",
    "# Clean the list (remove spaces)\n",
    "selected_pdfs = [pdf.strip() for pdf in selected_pdfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e752faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text.pdf', 'Text2.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(selected_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "710be62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'Text.pdf', 'page_no': 19}, page_content='🧪 Summary\\n\\nNormally, packets are auto-constructed by the OS.\\n\\nPacket crafting gives you full control over packet content and structure.\\n\\nIt is useful for security testing, penetration testing, research, and learning protocols deeply.\\n\\nTools like packETH (GUI) and hping (CLI) help craft packets without writing code.\\n\\n1. hyping'), Document(metadata={'source': 'Text.pdf', 'page_no': 19}, page_content='hping – Swiss Army Knife of TCP/IP Packet Crafting\\n\\nWhat is hping ? hping is a command-line network tool used for:\\n\\nCrafting and sending custom TCP/IP packets\\n\\nNetwork scanning\\n\\nFirewall testing\\n\\nAdvanced pinging\\n\\nSecurity auditing'), Document(metadata={'source': 'Text.pdf', 'page_no': 10}, page_content='💣 What is Metasploit?\\n\\nMetasploit is an open-source penetration testing framework used by:\\n\\nEthical hackers\\n\\nSecurity researchers\\n\\nSystem admins\\n\\nIts main job is to identify, validate, and exploit vulnerabilities in computer systems.\\n\\n🔧 What Can You Do with Metasploit?'), Document(metadata={'source': 'Text2.pdf', 'page_no': 26}, page_content=\"activities.\\n\\nMeterpreter Command: clearev (Windows)\\n\\nLinux: Modify or delete log files, or stop the syslog process.\\n\\n2. Rootkits:\\n\\nRootkits are a collection of software tools that allow attackers to hide their presence by modifying system components like the kernel. A rootkit may filter results in the process\\n\\ntable, making malicious processes invisible. While Metasploit doesn't provide rootkits, attackers often use process encoding to evade detection by malware scanners. Another\\n\\nstrategy involves renaming executable files to look like system processes (e.g., naming a malicious executable lsass.exe on Windows).\\n\\n3. Process Injection:\\n\\nProcess injection involves inserting code into an existing, trusted process to hide malicious activities. This way, the malicious code runs under the guise of an innocent, existing\"), Document(metadata={'source': 'Text2.pdf', 'page_no': 8}, page_content='🧰 John the Ripper (JtR) – A Popular Password Cracking Tool\\n\\nModes of Cracking:\\n\\n1. Single Crack Mode:\\n\\nFastest mode.\\n\\nUses username and other fields in the file, with mangling rules (modifying inputs to guess common password variants).'), Document(metadata={'source': 'Text2.pdf', 'page_no': 20}, page_content='Obfuscation Techniques:\\n\\nTOKEN – Break commands into less detectable tokens.\\n\\nAST – Modify the Abstract Syntax Tree.\\n\\nSTRING , ENCODING , COMPRESS , LAUNCHER – Varying ways to hide intentions.\\n\\n4. Payload Obfuscation\\n\\nTools like Metasploit support payload encoding to evade antivirus.\\n\\nThe idea is to hide the malicious intent until it executes.\\n\\n2. Privilege Escalation\\n\\nThis passage thoroughly explains privilege escalation—a critical phase in post-exploitation where an attacker, having already compromised a system, seeks to gain higher-level permissions, typically root or SYSTEM-level access, to maximize control.')]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6,\"filter\": {\"source\": {\"in\":selected_pdfs}}})\n",
    "print(retriever.invoke(\"What is hyping?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c82e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted_chunks = []\n",
    "    for doc in docs:\n",
    "        formatted_chunks.append({\n",
    "            \"content\": doc.page_content.strip(),\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"page_number\": doc.metadata.get(\"page_no\", \"?\")\n",
    "        })\n",
    "    return formatted_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d2715865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "316f65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def generate_answer(question):\n",
    "    # formatted_context = format_docs(selected_docs)\n",
    "    cohere_llm = Cohere(model=\"command\", temperature=0.1, cohere_api_key = os.getenv('cohere_api_key'))\n",
    "    \n",
    "    # print(prompt)\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | cohere_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9de9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\n",
      "{\n",
      "\"answer\": \"msfconsole: main command-line interface\",\n",
      "\"source\": \"Text.pdf\",\n",
      "\"page_number\": 11\n",
      "},\n",
      "{\n",
      "\"answer\": \"Modules: reusable components (exploits, scanners, payloads)\",\n",
      "\"source\": \"Text2.pdf\",\n",
      "\"page_number\": 4\n",
      "},\n",
      "{\n",
      "\"answer\": \"Payloads: code that runs after exploiting a system\",\n",
      "\"source\": \"Text.pdf\",\n",
      "\"page_number\": 11\n",
      "},\n",
      "{\n",
      "\"answer\": \"Encoders & NOPs: help avoid detection by antivirus systems\",\n",
      "\"source\": \"Text2.pdf\",\n",
      "\"page_number\": 4\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# question = input(\"Enter your question: \")\n",
    "ans = generate_answer(\"Key Components of Metasploit?\")\n",
    "print(ans)\n",
    "# with open('answer.txt', 'w') as f\n",
    "#     f.write(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
